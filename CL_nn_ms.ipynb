{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:19.288493Z",
     "start_time": "2024-08-12T14:11:18.548424Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(threshold=10_000)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:22.847579Z",
     "start_time": "2024-08-12T14:11:20.134526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df1 = pd.read_csv('datasets/bcsc_risk_factors_expanded1.csv')\n",
    "df2 = pd.read_csv('datasets/bcsc_risk_factors_expanded2.csv')\n",
    "df3 = pd.read_csv('datasets/bcsc_risk_factors_expanded3.csv')"
   ],
   "id": "857d5b3ae78103e4",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:23.164964Z",
     "start_time": "2024-08-12T14:11:22.848744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#slight cleaning\n",
    "df = pd.concat([df1, df2, df3])\n",
    "df = df[df.ne(9).all(1)] #drop unknowns (9s)\n",
    "df.drop(['year'], axis=1, inplace=True)"
   ],
   "id": "417926495bd748b2",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:23.893081Z",
     "start_time": "2024-08-12T14:11:23.879183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history_counts = df.breast_cancer_history.value_counts()\n",
    "history_counts[1]/history_counts.sum()"
   ],
   "id": "6dadcaff73f4178b",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## We have about 5.7% 1s, this is a rare event",
   "id": "a748d7b45447c4d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pytorch NN, Confusion Matrix, and Predict Proba",
   "id": "7ef314d5cb1c0e0f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:25.453305Z",
     "start_time": "2024-08-12T14:11:25.450715Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X = df.iloc[:, 0:df.shape[1]-1]\n",
    "y = df.iloc[:, df.shape[1]-1]"
   ],
   "id": "54751d49ad0aa096",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:30.318679Z",
     "start_time": "2024-08-12T14:11:26.075721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Balance the dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "X_test_balanced, y_test_balanced = smote.fit_resample(X_test_scaled, y_test)"
   ],
   "id": "4c38ea11d3fe042",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:30.328300Z",
     "start_time": "2024-08-12T14:11:30.319892Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_df = pd.DataFrame(y_train_balanced)\n",
    "y_df.breast_cancer_history.value_counts()"
   ],
   "id": "e534d729da8d7dc4",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:30.336141Z",
     "start_time": "2024-08-12T14:11:30.329104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# convert data to Pytorch tensors and move it to GPU\n",
    "X_train = T.tensor(X_train_balanced, dtype=T.float32)\n",
    "y_train = T.tensor(y_train_balanced.values, dtype=T.float32)\n",
    "X_test = T.tensor(X_test_scaled, dtype=T.float32)\n",
    "y_test = T.tensor(y_test.values, dtype=T.float32)"
   ],
   "id": "5d313fe74b6685c5",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:33.629501Z",
     "start_time": "2024-08-12T14:11:33.624990Z"
    }
   },
   "cell_type": "code",
   "source": "y_test",
   "id": "d3b6710b757dfc14",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:34.274399Z",
     "start_time": "2024-08-12T14:11:34.270635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super(NeuralNet, self).__init__()\n",
    "    self.hidden1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.hidden2 = nn.Linear(hidden_size, input_size)\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.output = nn.Linear(input_size, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu1(self.hidden1(x))\n",
    "    x = self.relu2(self.hidden2(x))\n",
    "    x = self.sigmoid(self.output(x))\n",
    "    #out = self.fc1(x)\n",
    "    #out = self.relu(out)\n",
    "    #out = self.fc2(out)\n",
    "    #out = self.sigmoid(out)\n",
    "    return x"
   ],
   "id": "56b570598107b29b",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:34.815225Z",
     "start_time": "2024-08-12T14:11:34.812158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Deep, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, input_size)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(input_size, hidden_size)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ],
   "id": "5a6a781fcd5dc47d",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:35.442019Z",
     "start_time": "2024-08-12T14:11:35.438422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(10, 64) \n",
    "        self.layer_2 = nn.Linear(64, 64)\n",
    "        self.layer_out = nn.Linear(64, 1) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.relu(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ],
   "id": "b91aaa585635ddee",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:35.906350Z",
     "start_time": "2024-08-12T14:11:35.903662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()\n",
    "    self.hid1 = nn.Linear(10, 8)  # 4-(8-8)-1\n",
    "    self.hid2 = nn.Linear(8, 8)\n",
    "    self.oupt = nn.Linear(8, 1)\n",
    "    nn.init.xavier_uniform_(self.hid1.weight)\n",
    "    nn.init.zeros_(self.hid1.bias)\n",
    "    nn.init.xavier_uniform_(self.hid2.weight)\n",
    "    nn.init.zeros_(self.hid2.bias)\n",
    "    nn.init.xavier_uniform_(self.oupt.weight)\n",
    "    nn.init.zeros_(self.oupt.bias)\n",
    "  def forward(self, x):\n",
    "    z = T.tanh(self.hid1(x))\n",
    "    z = T.tanh(self.hid2(z))\n",
    "    z = T.sigmoid(self.oupt(z))  # necessary\n",
    "    return z"
   ],
   "id": "c0721abed558a269",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:36.385131Z",
     "start_time": "2024-08-12T14:11:36.382365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Batcher:\n",
    "  def __init__(self, num_items, batch_size, seed=0):\n",
    "    self.indices = np.arange(num_items)\n",
    "    self.num_items = num_items\n",
    "    self.batch_size = batch_size\n",
    "    self.rnd = np.random.RandomState(seed)\n",
    "    self.rnd.shuffle(self.indices)\n",
    "    self.ptr = 0\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "  def __next__(self):\n",
    "    if self.ptr + self.batch_size > self.num_items:\n",
    "      self.rnd.shuffle(self.indices)\n",
    "      self.ptr = 0\n",
    "      raise StopIteration  # exit calling for-loop\n",
    "    else:\n",
    "      result = self.indices[self.ptr:self.ptr+self.batch_size]\n",
    "      self.ptr += self.batch_size\n",
    "      return result"
   ],
   "id": "e357326db2cb2874",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:37.207227Z",
     "start_time": "2024-08-12T14:11:37.203532Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def akkuracy(model, data_x, data_y):\n",
    "  # data_x and data_y are numpy array-of-arrays matrices\n",
    "  X = data_x\n",
    "  Y = data_y   # a Tensor of 0s and 1s\n",
    "  oupt = model(X)            # a Tensor of floats\n",
    "  pred_y = oupt >= 0.5       # a Tensor of 0s and 1s\n",
    "  num_correct = T.sum(Y==pred_y)  # a Tensor\n",
    "  acc = (num_correct.item() * 100.0 / len(data_y))  # scalar\n",
    "  return acc"
   ],
   "id": "c851cc44756c44",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:39.787511Z",
     "start_time": "2024-08-12T14:11:39.784908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#define hyperparameters\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 164\n",
    "learning_rate = .001\n",
    "num_epochs = 100\n",
    "bat_size = 16"
   ],
   "id": "98e147e244d72e52",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:41.856299Z",
     "start_time": "2024-08-12T14:11:41.854489Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "6039c0694a005351",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:42.410840Z",
     "start_time": "2024-08-12T14:11:42.407819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize the neural network and move it the GPU\n",
    "#model = NeuralNet(input_size, hidden_size)\n",
    "#model = Deep(input_size, hidden_size)\n",
    "#model = BinaryClassification()\n",
    "model = Net()"
   ],
   "id": "6b13f1d30d902778",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T14:11:43.273503Z",
     "start_time": "2024-08-12T14:11:42.900470Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.BCELoss()\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "27c9a25c991bd5d9",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-08-12T14:11:44.464734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "losses = []\n",
    "running_loss = 0.0\n",
    "\n",
    "model.train()\n",
    "n_items = len(X_train)\n",
    "batcher = Batcher(n_items, bat_size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    if epoch > 0 and epoch % 2 == 0:\n",
    "        print(\"epoch = %6d\" % epoch, end=\"\")\n",
    "        print(\"  batch loss = %7.4f\" % loss.item(), end=\"\")\n",
    "        acc = akkuracy(model, X_train, y_train)\n",
    "        print(\"  accuracy = %0.2f%%\" % acc)\n",
    "    for curr_batch in batcher:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train[curr_batch])\n",
    "        loss = criterion(outputs, y_train[curr_batch].unsqueeze(1))\n",
    "  \n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "print(\"Training complete \\n\")\n",
    "# 4. evaluate model\n",
    "net = model.eval()  # set eval mode\n",
    "acc = akkuracy(net, X_train, y_train)\n",
    "print(\"Accuracy on test data = %0.2f%%\" % acc)\n",
    "\n",
    "        # calculate accuracy\n",
    "  #  with T.no_grad():\n",
    "  #      predicted = outputs.round()\n",
    "  #      T.set_printoptions(threshold=10_000)\n",
    "  #          #print(predicted.abs().sum().item() == 0)\n",
    "  #          #print(predicted)\n",
    "  #      correct = (predicted == y_train[curr_batch].view(-1,1)).float().sum()\n",
    "  #      accuracy = correct/y_train[curr_batch].size(0)\n",
    "  #  net = net.eval()  # set eval mode\n",
    "  #  acc = akkuracy(net, test_x, test_y)  \n",
    "  #  #print(model.predict([1.0]))\n",
    "  #  print(f'Epoch [{epoch+1}/{num_epochs}], Loss : {loss.item():.4f}, Accuracy: {accuracy.item() * 100:.2f}%')"
   ],
   "id": "2e4843a346c2346a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T13:27:44.753561Z",
     "start_time": "2024-08-12T13:27:44.606019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.plot(losses)\n",
    "plt.title('loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.savefig('loss_vs_epochs.png')\n"
   ],
   "id": "8621a941f76b8f78",
   "execution_count": 1908,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T13:27:46.191695Z",
     "start_time": "2024-08-12T13:27:46.189496Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cc74090be4ac8a3b",
   "execution_count": 1908,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T13:28:02.710926Z",
     "start_time": "2024-08-12T13:28:02.474055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "with T.no_grad():\n",
    "  outputs = model(X_train)\n",
    "  predicted = outputs.round()\n",
    "  correct = (predicted == y_train.view(-1,1)).float().sum()\n",
    "  accuracy = correct/y_train.size(0)\n",
    "  print(f'Accuracy on training data: {accuracy.item() * 100:.2f}%')"
   ],
   "id": "5ef8bdd6049aa3ef",
   "execution_count": 1910,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T13:28:11.842577Z",
     "start_time": "2024-08-12T13:28:11.655201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# evaluation on test set\n",
    "model.eval()\n",
    "with T.no_grad():\n",
    "  outputs = model(X_test)\n",
    "  predicted = outputs.round()\n",
    "  correct = (predicted == y_test.view(-1,1)).float().sum()\n",
    "  accuracy = correct/y_test.size(0)\n",
    "  print(f'Accuracy on test data: {accuracy.item() * 100:.2f}%')"
   ],
   "id": "a9eeadd65ae3b4c2",
   "execution_count": 1911,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T13:28:22.206690Z",
     "start_time": "2024-08-12T13:28:22.201807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model_f, X_test_tensor_f, y_test_tensor_f):\n",
    "    # Set the model to evaluation mode\n",
    "    model_f.eval()\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        y_pred = model_f(X_test_tensor_f)\n",
    "        y_pred_class = y_pred.round()\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    y_true = y_test_tensor_f.numpy()\n",
    "    y_pred = y_pred_class.numpy()\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    #plt.show()\n",
    "    plt.savefig('Confusion_Matrix.png')\n",
    "\n",
    "    # Print classification report\n",
    "    from sklearn.metrics import classification_report\n",
    "    print(classification_report(y_true, y_pred))"
   ],
   "id": "187b3c519efdd7e5",
   "execution_count": 1912,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T13:28:24.329033Z",
     "start_time": "2024-08-12T13:28:23.390446Z"
    }
   },
   "cell_type": "code",
   "source": "plot_confusion_matrix(model, X_test, y_test)\n",
   "id": "aab405c106d0c6",
   "execution_count": 1913,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T04:41:15.225479Z",
     "start_time": "2024-08-12T04:41:15.216195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_proba(model, features):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        # Convert features to a PyTorch tensor\n",
    "        if isinstance(features, np.ndarray):\n",
    "            features = torch.FloatTensor(features)\n",
    "        elif isinstance(features, pd.DataFrame):\n",
    "            features = torch.FloatTensor(features.values)\n",
    "        \n",
    "        # Ensure features is 2D\n",
    "        if features.dim() == 1:\n",
    "            features = features.unsqueeze(0)\n",
    "        \n",
    "        # Get the raw output (logits) from the model\n",
    "        logits = model(features)\n",
    "        \n",
    "        # Apply sigmoid to get probabilities\n",
    "        probs = torch.sigmoid(logits)\n",
    "        \n",
    "        # Return probabilities for both classes\n",
    "        return torch.cat([1 - probs, probs], dim=1)"
   ],
   "id": "74fe336c99a443e7",
   "execution_count": 1686,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T04:46:52.352121Z",
     "start_time": "2024-08-12T04:46:52.347060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# For a single row\n",
    "single_row = X.iloc[0]  # Get the first row of your features\n",
    "scaled_row = scaler.transform(single_row.values.reshape(1, -1))\n",
    "probabilities = predict_proba(model, scaled_row)\n",
    "print(f\"Probabilities for single row: {probabilities.numpy()}\")\n"
   ],
   "id": "d25b51ecf092f1d4",
   "execution_count": 1693,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T04:54:19.549324Z",
     "start_time": "2024-08-12T04:54:19.543880Z"
    }
   },
   "cell_type": "code",
   "source": "y_df = pd.DataFrame(y, columns=['breast_cancer_history']) ",
   "id": "1eb831ba6495d9b2",
   "execution_count": 1702,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T04:54:37.124380Z",
     "start_time": "2024-08-12T04:54:37.111421Z"
    }
   },
   "cell_type": "code",
   "source": "y_df.index[y_df['breast_cancer_history'] == 1].tolist()",
   "id": "48a7e556fe334d0e",
   "execution_count": 1703,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T04:55:28.744155Z",
     "start_time": "2024-08-12T04:55:28.736938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "single_row = X.iloc[71960]\n",
    "print(single_row)\n",
    "scaled_row = scaler.transform(single_row.values.reshape(1, -1))\n",
    "probabilities = predict_proba(model, scaled_row)\n",
    "\n",
    "print(\"Probabilities for single row:\")\n",
    "print(f\"Class 0 (No breast cancer history): {probabilities[0, 0].item():.4f}\")\n",
    "print(f\"Class 1 (Breast cancer history): {probabilities[0, 1].item():.4f}\")\n",
    "\n",
    "# Determine the predicted class\n",
    "predicted_class = probabilities.argmax(dim=1).item()\n",
    "print(f\"\\nPredicted class: {predicted_class}\")\n",
    "\n",
    "# Compare to actual target\n",
    "actual_class = y.iloc[71960]\n",
    "print(f\"Actual class: {actual_class}\")\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "if predicted_class == 1:\n",
    "    print(f\"The model predicts a {probabilities[0, 1].item()*100:.2f}% chance of breast cancer history.\")\n",
    "else:\n",
    "    print(f\"The model predicts a {probabilities[0, 0].item()*100:.2f}% chance of no breast cancer history.\")"
   ],
   "id": "ee7db87705732e28",
   "execution_count": 1706,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T05:00:41.173028Z",
     "start_time": "2024-08-12T05:00:41.168503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_feature_prediction_df(model, X_row, y_actual, scaler):\n",
    "    \"\"\"\n",
    "    Create a DataFrame containing the features, predicted probabilities, and actual value for a single row.\n",
    "    \n",
    "    :param model: Trained PyTorch model\n",
    "    :param X_row: Single row of features (pandas Series or DataFrame)\n",
    "    :param y_actual: Actual target value\n",
    "    :param scaler: Fitted StandardScaler used to preprocess the data\n",
    "    :return: pandas DataFrame\n",
    "    \"\"\"\n",
    "    # Ensure X_row is a DataFrame\n",
    "    if isinstance(X_row, pd.Series):\n",
    "        X_row = X_row.to_frame().T\n",
    "    \n",
    "    # Scale the features\n",
    "    X_scaled = scaler.transform(X_row)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        X_tensor = torch.FloatTensor(X_scaled)\n",
    "        logits = model(X_tensor)\n",
    "        probs = torch.sigmoid(logits)\n",
    "    \n",
    "    # Create a dictionary to store all data\n",
    "    data = {}\n",
    "    \n",
    "    # Add features to the dictionary\n",
    "    for col in X_row.columns:\n",
    "        data[col] = X_row[col].values[0]\n",
    "    \n",
    "    # Add predicted probabilities\n",
    "    data['Prob_No_History'] = (1 - probs).item()\n",
    "    data['Prob_History'] = probs.item()\n",
    "    \n",
    "    # Add actual value\n",
    "    data['Actual_Value'] = y_actual\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame([data])\n",
    "    \n",
    "    return df\n"
   ],
   "id": "6a1af1300b154e5c",
   "execution_count": 1710,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T05:01:24.069271Z",
     "start_time": "2024-08-12T05:01:23.646367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result_dfs = []\n",
    "for i in range(500):\n",
    "    X_single = X.iloc[i]\n",
    "    y_single = y.iloc[i]\n",
    "    df = create_feature_prediction_df(model, X_single, y_single, scaler)\n",
    "    result_dfs.append(df)\n",
    "\n",
    "# Combine all results into a single DataFrame\n",
    "all_results = pd.concat(result_dfs, ignore_index=True)\n"
   ],
   "id": "ac1fe19fd6d9d555",
   "execution_count": 1712,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T05:01:29.936906Z",
     "start_time": "2024-08-12T05:01:29.929708Z"
    }
   },
   "cell_type": "code",
   "source": "all_results",
   "id": "d1acbd8bbbf32cdf",
   "execution_count": 1713,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "110733e318c6eabc",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
